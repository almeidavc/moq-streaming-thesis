% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Background}\label{chapter:background}

\section{Video coding standards and formats}
% TODO: Add an introdutory sentence explaining that i made simplifying assumptions.
Raw uncompressed video is too large to be transmitted over the network. Video compression algorithms reduce the size of video content by reducing the redundancy in video frames. Two popular codings include H.264/AVC \parencite{wiegandOverview264AVC2003} and H.265/HEVC \parencite{sullivanOverviewHighEfficiency2012}. Encoded video consists of a sequence of independently decodable units called \acp{GoP}. A \ac{GoP} contains intra-coded pictures or I-frames, predicted pictures or P-frames, and bi-directional predicted pictures or B-frames. An I-frame is a fully self-contained image; P-frames reference previous frames, and B-frames reference both previous and future frames. Since B-frames reference future frames, the encoder can only produce them after the future frames that are referenced.

% TODO: Should I talk about AVC nal units and slices here? Or when I talk about parsing the AVC stream? Should I even talk about parsing the AVC stream?

The compressed video content can be stored in several formats. MP4 \parencite{mpeg-4part14MPEG4MP4File}, formally known as MPEG-4 Part 14, is the most commonly used container format to store multimedia content consisting of audio and video streams. The audio and video streams each correspond to a track in the MP4 format. MP4 uses a box model. A typical video file contains a \textit{ftyp} box, identifying the compatible file type specifications, a \textit{mdat} box, containing the actual audio or video payload, and a \textit{moov} box, describing the audio samples or video frames in the mdat box. The moov box contains information relevant to the media content as a whole, such as which tracks it consists of, the codec used, and information for every frame, such as the size, timestamp, or duration of the frame.

The regular file structure of an MP4 file is not suitable for live streaming because the moov box, which describes every frame in the media content, cannot be produced until all frames have become available. Fragmented MP4 packages media in a format suitable for live streaming, by dividing the media into chunks called fragments, which can be produced independently. These fragments are pairs of \textit{moof} and \textit{mdat} boxes. The moof box is similar to the moov box, except that it only describes the payload of the associated mdat box. The moov box in fragmented MP4 is used to describe information common to all audio or video frames, such as the codec used. Fragments can contain a varying number of frames depending on the packager configuration.

\section{HTTP-based live streaming}
HTTP-based media streaming is the most popular streaming method today by far. Using a pull-based approach, the client progressively downloads the media stream from an HTTP server. For this purpose, the media stream is split into segments, which are a few seconds long. At the beginning of a session, the client fetches a manifest file that describes the video segments in detail, containing information such as the codec, resolution, and duration of each segment. The manifest file contains all the information necessary for the client to request and play the video segments. The client then continuously requests the video segments from the server and plays them.

Furthermore, with \acf{HAS}, the server reencodes the video segments into multiple resolutions and bitrates using a bitrate ladder in order to offer the media stream in multiple qualities. During playback, the client monitors the network throughput to decide which bit rate to choose for the next video segments. This allows the client to request lower quality segments if the connection doesn't have enough bandwidth or increase the quality of the stream if the device is capable and if there is enough bandwidth. This process is called \ac{ABR} Streaming.

The most popular \ac{HAS}-based methods today are \ac{DASH} \parencite{DASH} and \ac{HLS} \parencite{incHTTPLiveStreaming}. Although similar at a high level, each defines its own format for media segments and manifest files.

HTTP-based live streaming is the dominant streaming method due to the nature of the web infrastructure and HTTP itself. First, HTTP-based live streaming can scale massively because it can leverage existing infrastructures and caches of \acp{CDN}. Furthermore, scale is much easier because HTTP is stateless, and all the logic for controlling a session resides in the client, resulting in simple servers and relays. Second, the widespread deployment of HTTP makes an HTTP-based live streaming system easy to deploy. Finally, HTTP-based systems are much cheaper than custom push-based approaches due again to the widespread deployment of HTTP.

% TODO: Explain that delays add up and cite durakEvaluatingPerformanceApple2020
% TODO: Maybe explain how the initial solutions worked and cite them: Low latency live video streaming using HTTP chunked encoding, Low Latency Live Video Streaming over HTTP 2.0
However, the first versions of \ac{HAS} streaming protocols, such as \ac{DASH} and \ac{HLS}, did not support low latency. The reason for this is that a video segment could not be delivered until it was fully generated because it wasn't packaged until all the frames in it were produced. This results in latencies that are at least the segment duration, which is a couple of seconds. Decreasing the segment duration does decrease the minimum latency, but it is not viable past a certain point since it increases the number of requests the client needs to perform, impacting the performance of HTTP servers and caches. In addition, decreasing the segment duration also decreases the encoding efficiency.

% TODO: Find something I can cite for LL-DASH and LL-HLS
To address the low-latency requirements that became increasingly more common, low-latency extensions of \ac{DASH} and \ac{HLS}, namely LL-DASH and LL-HLS, were developed. These extensions leverage the \ac{CMAF} \parencite{CMAF}, developed in 2016 to standardize the format of media segments, which breaks segments down into chunks. In essence, the principle behind LL-DASH and LL-HLS is to encode, package, and deliver segments in smaller chunks \parencite{bentalebOneSecondLatencyEvolution2023, durakEvaluatingPerformanceApple2020}. The encoder makes frames available to the packager immediately after encoding them; the packager, in turn, packages them into \ac{CMAF} chunks, containing a couple of frames at most. Finally, the chunks are delivered to the client as soon as they become available. In summary, with chunked encoding, packaging, and delivery, the latency is no longer determined by the segment duration.

\section{QUIC and Media over QUIC}
QUIC \parencite{iyengarQUICUDPBasedMultiplexed2021} is a connection-oriented transport protocol built on top of \acs{UDP}, providing an alternative to TCP. QUIC provides independent streams over a single multiplexed connection, which, unlike streams multiplexed over a TCP connection, don't suffer from \ac{HOL} blocking. Furthermore, streams can be prioritized and terminated, giving applications much more control over the transmission of their data. Other features include a faster handshake process, improved performance during network-switching events by using a unique connection identifier, and support for unreliable datagrams.

Systems using TCP may notice some improvements by simply switching to QUIC. QUIC provides lower startup, and seeking delays by starting media streams more quickly, and handles network switching events more gracefully, providing a better \ac{QoE} for users that are mobile \parencite{arisuQuicklyStartingMedia2018}. In addition, QUIC shows a reduced number of stalls and lower stall durations in lossy networks than TCP, providing a better \ac{QoE} in environments where packet loss is frequent \parencite{shreedharEvaluatingQUICPerformance2022}.

Nevertheless, transitioning from TCP to QUIC does not result in major improvements out of the box. In network environments with low packet loss, \ac{HAS}-based applications using QUIC do not perform better than their TCP counterparts \parencite{timmererAdvancedTransportOptions2016}. In order to achieve lower latencies and improvements in \ac{QoE}, custom application-layer protocols need to be developed that fully leverage QUIC's features \parencite{nguyenTakeRedPill2022}.

\ac{MoQ} is a relatively recent application-layer protocol designed for low-latency streaming of media. It is designed for various applications such as live streaming, gaming, and media conferencing. An \ac{IETF} working group was formed in 2022 \parencite{MediaQUICMoq}. 

\ac{MoQ} was designed with a couple of goals in mind to address the challenges with traditional streaming protocols. \ac{MoQ} aims to define a single transport protocol for media ingestion and distribution, eliminating the need to repackage media at multiple stages of the streaming pipeline. In addition, \ac{MoQ} is meant to be highly scalable by designing the protocol with first-class support for relays. 

Furthermore, \ac{MoQ} has the potential to achieve lower latencies than the traditional streaming protocols. First, applications are able to map media to multiple QUIC streams, which don't suffer from \ac{HOL} blocking. Second, \ac{MoQ} enables new ways to respond to congestion by leveraging QUIC. When the network conditions are not ideal, the latency of a protocol is determined by how fast it can detect and respond to congestion \parencite{curleyMediaQUICTransport2024}. \ac{MoQ} enables new ways to respond to congestion by leveraging QUIC. Using stream prioritization, applications can prioritize the delivery of the most crucial media. Furthermore, applications can drop media by terminating streams to save bandwidth for their most important media. The protocol is flexible, allowing applications to choose whether to prioritize latency or quality.

The \ac{MOQT} protocol is based on a publish/subscribe workflow. Producers publish media that clients can subscribe to. Relays simply forward media, providing the link between publishers and subscribers. \ac{MoQ} represents media using an object model. An object is the smallest unit of data in \ac{MOQT}, which in the video use case corresponds to the video frames. At the application level, objects might depend on each other, meaning the application can't process object X without having object Y. Groups in \ac{MOQT} contain objects that depend on each other, which themselves are independent. Groups provide a join point for new subscriptions. A typical configuration maps \acp{GoP} to groups. Finally, groups belong to tracks. A track is simply a sequence of groups that clients can subscribe to. \ac{MoQ} is intended to be flexible and therefore leaves the specifics of how the media content is mapped to these primitives up to the application. 


