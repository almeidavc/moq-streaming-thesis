% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Implementation}\label{chapter:implementation}

% TODO: fix this. make it congruent with the first point in the list of contributions
We now turn to the base implementation of our prototype live streaming system. Our prototype uses draft version 3 of \ac{MOQT}, but the main points outlined in this section should be applicable to versions 4 and 5, the latest version at the time of writing.

We first describe our base implementation. Similar to a traditional live-streaming system that uses TCP, this version transmits the frames reliably and in order, using a single QUIC stream. We also use this section to describe the high-level architecture of the system and the inner workings of the client and server components, which all our approaches share. We start with the server implementation and then we proceed with the client.

\section{Origin server}
The server ingests a live video stream and broadcasts it to clients. Our prototype ingests a live video stream from \lstinline{stdin}, which is encoded with H.264/AVC and packaged in MP4 fragments. The stream is fragmented at every frame with the ffmpeg option \lstinline{frag_every_frame}. % TODO: cite https://ffmpeg.org/ffmpeg-formats.html#Options-6

The server ingests and parses the livestream, serving subscribed clients, and concurrently listens for new subscription requests. To serve the clients, the server provides two tracks: an "init" track and a "video" track. The init track is used to serve the ftyp and moov boxes, which the server parses and stores in the beginning of the ingestion process. Clients subscribe to the init track on stream startup to configure the decoder. % make an analogy to the DASH manifest or HLS playlist

The video track is used to serve the actual video content. The server parses the MP4 fragments, which correspond to the pairs of moof and mdat boxes, that follow the ftyp and moov boxes, and fans them out to subscribers. Each fragment, which contains exactly one frame, is sent as a MoQ object on the "video" track. The server groups together MoQ objects that belong to the same GoP into the same MoQ group, by incrementing the group number for each keyframe, to provide a join point for new subscriptions. The baseline version uses a single stream for all objects in the video track.

\section{Live streaming client}\label{section:baseline_client}
% TODO: make sure to explain what the render pipeline is, and mention calculateTimeUntilFrame
We now turn our attention to the client.

Subscribing and playing a live stream works as follows. First, the client downloads the ftyp and moov boxes, by subscribing to the init track. These boxes contain metadata about the tracks in the MP4 stream % TODO: such as...
that the client will later need to parse the mp4 fragments and decode the video frames.

When the user clicks "play", the client subscribes to the video track. Since the player can't start playback in the middle of a GoP, the client can choose between two different locations for which the subscription should start. The client can either wait for the new GoP to be produced or subscribe to the latest GoP. Assuming a GoP to MoQ group mapping, like we do in our server, in MoQ draft 3 we can achieve the former by using the Subscribe location mode RelativeNext and value 0 and to achieve the latter, we use mode RelativePrevious and value 0. Waiting for the next GoP to start ensures the lowest possible latency, while starting playback at the latest GoP keeps startup delays low but it can lead to latencies up to the duration of GoPs. % TODO: this is not really true, because we can simply subscribe to the latest gop and flush all frames until the last frame or something like this
Whether to prioritize startup delay or latency is up to decide based on the use case.

As the client receives the MP4 fragments from the video track, it adds them to a pipeline that processes the raw fragments, extracting the video frames and eventually rendering them. The pipeline consists of three stages. 

First, the player extracts the frame payload and frame metadata such as the frame duration, decode timestamp and presentation timestamp from the mp4 fragment. 

Then we decode the encoded frame using the frame metadata with the VideoDecoder from the WebCodecs API. We configure the decoder using the ftyp and moov boxes, which we downloaded from the init track.  To configure the VideoDecoder, two options are worth mentioning. We enable \lstinline{optimizeForLatency} % TODO: What does optimizeForLatency actually do
and set \lstinline{hardwareAcceleration} to \lstinline{"prefer-software"}. Regarding the latter, with the default setting, the VideoDecoder was occasionally throwing errors with the message "Decoding error". With \lstinline{"prefer-software"} these errors did not occur, although we can't fully explain the reason behind it. % TODO: Elaborate. E.g. These errors could be caused by A deeper issue with our implementation could We are not sure if this is an issue with our implementation or an issue with the WebCodecs API.

Finally, we render the decoded frame, by drawing it to a canvas element. Rendering frames as they are received results in jerky playback, and consequently poor \ac{QoE}. To provide smooother playback, we use a jitter buffer and time the rendering of frames, rather than rendering the frames immediately after decoding them. After a frame is received and decoded, it is added to the jitter buffer. Once the buffer size reaches the target size, the player start consuming frames from the buffer. While the buffer has frames, the player continually retrieves the frame with the lowest timestamp from the buffer, and waits for the correct time to render it. If the buffer runs out of frames, the player starts the process of filling the buffer again.

% TODO: explain the algorithm. Explain that resumedPlayingAt is set to undefined when the player first starts, or rebuffers, and that it's set when the first frame is rendered after a pause, or rebuffer event
Algorithm \ref{alg:time_until_frame} shows how the player calculates the time until a frame is to be rendered. The time the player should wait depends on the current media time, which is the elapsed time since playback started in the current session. If the stream can be played without interruptions, the time until a frame is to be rendered is the difference between the frame timestamp and the media time. However, if the player rebuffers, using the absolute media time would cause all frames that are lagging behind to be rendered immediately after the player resumes playback at the same. If the network conditions that led to the rebuffering event are not short-lived, then the player would continuously flush all frames, emptying the buffer, and start buffering again. We handle this issue, by timing the rendering of frames relative to the point in time, at which playback resumed after a rebuffering event, rather than the absolute playback start.  % TODO: However if we do this, it increases latency. Further work needs to be done on this. Find a hybrid strategy

\begin{algorithm}
\caption{Calculate time to render frame}\label{alg:time_until_frame}
\begin{algorithmic}
\Function{calculateTimeUntilFrame}{$frameTimestamp$}
    \State $now \gets \Call{now}{\null}$
    \item[]
    \If{$resumedPlayingAt = undefined$}
        \State $resumedPlayingAt \gets \textnormal{\{\}}$
        \State $resumedPlayingAt.localTime \gets now$
        \State $resumedPlayingAt.mediaTime \gets frameTimestamp$
    \EndIf
    \item[]
    \State $relativeMediaTime \gets now - resumedPlayingAt.localTime$
    \State $relativeFrameTimestamp \gets (frameTimestamp - resumedPlayingAt.mediaTime) / 1000$
    \State \Return $\max(0, relativeFrameTimestamp - relativeMediaTime)$
\EndFunction
\end{algorithmic}
\end{algorithm}


\chapter{Deprioritizing B-frames}\label{chapter:deprioritizing_b_frames}
In order to prevent the latency from increasing during network congestion, the server needs to send less data to the client. One way to reduce the stream's bitrate is by dropping B-frames. The reason this works is twofold. 

First, B-frames are not usually depended by other frames, and therefore they can be dropped without affecting the decodability of other frames. We will assume for the remainder of this section that B-frames are not used as references. We note however that state of the art encoders allow the use of B-frames as references through B-pyramid schemes. We will discuss the use of reference B-frames, including ways to adapt our implementation to support them, towards the end of the section. % TODO: Can B-frames be used in low-latency livestreaming

Second, the P-, and I-frames alone form themselves a stream that is playable, albeit with video artifacts, such that we can drop all B-frames if necessary. This is only true, if the number of consecutive B-frames is limited, which is the case in low-latency live streaming. % TODO: explain why: since dropping too many frames in a row would result in an incoherent video. If this were not the case, then it would like we were skipping media. This is a viable strategy, but it's one that requires different ways to render the frames at the client. We will talk about this in the next section.

% TODO: cite https://www.ietf.org/id/draft-lcurley-moq-transfork-00.html#name-layers?
We effectively divide the stream into two layers: a base layer, consisting of the I-, and P-frames, and an enhancement layer containing all B-frames. During congestion, the server drops the enhancement layer, degrading the stream quality to reduce the stream's bitrate.


% TODO: find a better name for this section
\section{Implementation}
Our goal is to deprioritize the B-frames, such that they are sent on a best-effort basis. To accomplish this, we prioritize the layers as follows. The base layer is assigned the highest priority so that the server always transmits I-frames and P-frames first, if any are available. On the other hand, B-frames get a lower priority, such that they are transmitted only if there is enough network bandwidth.

We divide the stream into two MoQ tracks, one for each layer. In both tracks each GoP forms a group. The group boundaries must be aligned in this way to provide a join point for new subscriptions that is synchronized across both tracks. The server implements this by incrementing the groupId for each keyframe. At startup, the client subscribes concurrently to both tracks referencing the same MoQ group.

The track that serves the base layer uses a single QUIC stream to deliver the I- and P-frames in order. 
% TODO: mention that we are using quinn-rs and how it implements priorities
We assign the second highest priority to this stream. (The highest priority value is assigned to the
init track).

% TODO: Maybe use an excalidraw sketch to make the explanation more clear
To stream the enhancement layer, one could think of using a single stream with a lower priority. However, this simple prioritization strategy wouldn't have the desired effect. We demonstrate the issue with an example. Suppose that the network was congested and we didn't transmit any B-frames because there wasn't enough bandwidth. When the network recovers, we want to transmit the new B-frames. However the old B-frames are transmitted because they were queued first. If the network has just enough bandwidth to send the B-frames of one GoP, we would never get a chance to render the B-frames. They would always be late. The same reasoning applies to a stream per GoP for the B-frames. If the server gets enough bandwidth to transmit some B-frames towards the end of a GoP, we don't want to send the first frames of the GoP since they are useless. 

Each B-frame should have a higher priority than the previous B-frame that was produced. In general, frames in enhancement layers should be prioritized according to the new over old policy, since they are always transmitted on a best-effort basis. 

Since, in QUIC, the unit of prioritization is the stream, each B-frame needs to be sent in a separate stream. The decode timestamps of B-frames can be used as the priority for each stream, or if one wants to make optimal use of the number of bits, the current count of B-frames.

% TODO: fix this. a single frame can contains slices of all types
To prioritize I-, and P-frames over B-frames, the server must be able to distinguish frames of different types in the first place. Identifying the type of a frame is not as trivial as one might think. Parsing the mp4 fragments is not enough, because there doesn't exist any boxes in the mp4 container that contain this information. One needs to parse the encoded sample. For AVC encoded frames, for example, we first parse the \ac{NAL} units with the type 0, Coded slice of a non-IDR picture, and type 5, Coded slice of an IDR picture, from the mp4 sample. We then parse the slice types from the slice header of these NAL units.

Additionally, the server includes the frame type in the header of the payload of the MoQ object, such that clients, and potentially relays as well, can easily extract the type of a frame from the MoQ object without having to parse the encoded samples.

\section{Handling out-of-order frames}\label{section:out_of_order_frames}
% TODO (maybe): instead of explaining why they can arrive out-of-order, simply say that is best to assume they will arrive out-of-order because nothing guarantees that they will arrive in order when we use multiple streams
Because we are using multiple QUIC streams, which are independent and don't provide any ordering guarantees, frames will arrive out-of-order. Within the base layer, I- and P-frames will arrive in decode order, however B-frames can arrive out-of-order relative to the frames in the base layer. Furthermore, B-frames can arrive out-of-order relative to each other. First, if the network becomes temporarily congested such that some B-frames are not transmitted, then old B-frames will be sent to the client when the network recovers. Second, a B-frame can arrive ahead of another B-frame that was sent first, because the underlying packets take different network paths or due to packet loss. % TODO: cite source
In this subsection, we propose a solution to this issue. First, we explain the intuition behind our approach, and then we describe it more precisely, by explaining relevant implementation details.

% TODO: explain what the render pipeline is (in the baseline version section)
We add frames from the base layer, which arrive in order relative to each other, directly to the render pipeline. Although there might be discontinuities between these frames, we don't wait for the missing B-frames, because they might never arrive.

B-frames can be late or early. We define a B-frame to be late, if at the time of its arrival a frame with a higher timestamp has already arrived. Similarly we say a B-frame is early, if it arrived ahead of frames with a lower timestamp. If a B-frame is late, we drop it, because we don't want to decode and render frames out-of-order. If a B-frame is early and no later I-, or P-frames have arrived, we wait for the earlier frames to arrive. This is because the B-frame might have arrived ahead of a frame from the base layer, which we can't drop. Note that the B-frame might simply be ahead of other B-frames, but we have no way of telling, so we need to assume the worst case. If a B-frame is neither late nor early by our definitions, we add it directly to the render pipeline. 

In practice, frames that are in order are not added directly to the render pipeline. Instead frames are first added to a jitter buffer to handle small variations in the arrival of frames. Otherwise, if we add frames directly to the render pipeline, a frame that is late by only a few milliseconds will be dropped. From our experiments, we found a 100 to 200 milliseconds jitter buffer to be optimal. % optimal in what sense? explain

% TODO: double check correctness of the algorithm and reasoning
% TODO: explain that buffer works like a priority queue, and buffer[0] is the frame with the lowest decode timestamp and buffer[buffer.length - 1] is the frame with the highest timestamp
% Should we explain the bufferSize function?
% explain why you use a while loop instead of an if statement
Algorithm \ref{alg:reorder} shows pseudocode for the process of handling an incoming frame and retrieving the next frame to be enqueued to the render pipeline. In summary: first, we check if the frame is a B-frame and if the frame is late, by checking if it has a lower decode timestamp than the decode timestamp of the last enqueued frame. If the B-frame is late, we drop it. Otherwise, we add the frame to the jitter buffer. This buffer works like a min-heap with the decode timestamps serving as the key. Then, if the buffer size is greater than the target size, we \textit{try} to retrieve the next frames from the buffer and enqueue them to the render pipeline. To do this, we check the frame with the lowest timestamp in the buffer. If the frame is an I-, or P-frame or if there is no discontinuities between the frame's dts and the last enqueued frames dts, we enqueue the frame to the buffer and try to enqueue the next frame. If the frame is a B-frame and it is early, then we scan the buffer for an I-, or P-frame. If we find an I-, or P-frame, then it must be the case that the frame has a higher timestamp, and we enqueue the frame. Note that the B-frame cannot be ahead of any I-, or P-frames since frames from the base layer are transmitted in order. If, on the other hand, the buffer only has B-frames, then we wait for the missing frames to arrive by returning, since it might be ahead of I-, or P-frames.

\begin{algorithm}
\caption{Reorder Algorithm}\label{alg:reorder}
\begin{algorithmic}
\Function{reorder}{$frame$}
    \If{$frame.type = B \land lastEnqueued \neq null \land frame.dts < lastEnqueued.dts$}
        \State \Return \Comment{Drop late B-frames}
    \EndIf
    \item[]
    \State add $frame$ to $buffer$
    \item[]
    \While{$\Call{bufferSize}{\null} > targetSize$}
        \State $next \gets buffer[0]$
        \If{$next.type \neq B \lor lastEnqueued = null \lor next.dts = lastEnqueued.dts + lastEnqueued.duration$}
            \State enqueue $next$
            \State pop $buffer$
            \State $lastEnqueued \gets next$
            \State \textbf{continue}
        \EndIf
        \item[]
        \State $i \gets 1$ \Comment{B-frame is early}
        \While{$i < buffer.length \land buffer[i].frameType = B$}
            \State $i \gets i + 1$
        \EndWhile
        \If{$i < buffer.length$} \Comment{B-frame is not ahead of any I-, or P-frames}
            \State enqueue $next$
            \State pop $buffer$
            \State $lastEnqueued \gets next$
        \Else \Comment{Wait, because B-frame might be ahead of I-, or P-frames}
            \State \textbf{break} 
        \EndIf
    \EndWhile
\EndFunction
\item[]
\Function{bufferSize}{\null}
    \If{$buffer.length = 0$}
        \State \Return 0
    \EndIf
    \item[]
    \State $oldest \gets buffer[0]$
    \State $newest \gets buffer[buffer.length - 1]$
    \State \Return $newest.dts - oldest.dts$
\EndFunction
\end{algorithmic}
\end{algorithm}

\section{Reference B-frames}
We assumed until now that B-frames are not depended on by other frames. However, H.264 and newer video codings allow B-frames to be used as references for the encoding of other frames. Our current approach, which treats all B-frames as independently droppable, is incompatible with this setting. Recall that if we drop a B-frame, we also need to drop any frames that depend on it. Otherwise, the player would try to decode a frame for which it does not have its dependencies, which would break the player.

Linking B-frames that depend on each other so that if we drop a B-frame, we also drop any B-frames that depend on it, would add too much complexity. It would involve extracting the dependencies between B-frames, which requires parsing the encoded video samples, and implementing a media to streams mapping or prioritization scheme that groups frames and their dependencies together. For this reason, this solution is explicitly a non-goal for us.

One can simply disable the use of B-frames as references.  For H.264/AVC, one can control this with the b-pyramid setting. % TODO: cite https://en.wikibooks.org/wiki/MeGUI/x264_Settings#b-pyramid or other source
This would slightly decrease the compression rate. We believe this is acceptable in a low-latency live-streaming scenario, because we don't use that many B-frames anyways.

However the above solution might not be possible, if we are not in control of the encoder. Then, an alternative solution, is to only use B-frames that are not used as references in the enhancement layer. To distinguish reference B-frames from non-reference B-frames, one can try using the mp4 sample flags \lstinline{sample_depends_on} and \lstinline{sample_is_depended_on}. % TODO: cite mp4 spec
We note, however, that in our experiments, both flags were set to the value $0$, indicating that the dependencies were unknown, for the majority of frames. We don't know if this is a limitation of the library that we used to package the stream into mp4 or if it's something we didn't configure correctly. % Anyways, if for whatever reason these are also unknown for the reader, then you would have to parse the encoded sample and extract the frame dependencies that way, for example parse the pps if the codec is H.264, which might be prohibitively too complex.

\chapter{Skipping old media}\label{chapter:skipping_old_media}
Adapting a live stream's quality is an effective technique for prioritizing latency, but it can't handle large changes in the network bandwidth effectively alone. Consider the case where a viewer is watching a live stream on their phone and the available network bandwidth drops drastically and then recovers. This can happen because the viewer is watching the live stream on his phone on the go and temporarily goes past an area of bad coverage for example. While the network connectivity is poor, streaming a lower quality stream surely helps. However video will still lag behind and latency will increase, due to the low network throughput. Video will queue up at the server. When the network recovers, rather than playing all the video that queue up, we want to resume playback at the live edge. We want to skip large portions of media if we don't get a chance to transmit it due to network conditions.

% TODO: mention Warp
To achieve this, we prioritize new over old media. 

Specifically, we map and prioritize the video track as follows. We map each GoP of the video stream to a MoQ group, and send each group in its own QUIC stream. We prioritize new GoPs over old ones, by using the timestamp of the first frame in the group as the priority of the QUIC stream.

In the client, we drop any frames belonging to old GoPs. Handling late GoPs is similar to how we handled out-of-order frames in \autoref{section:out_of_order_frames}. One can keep track of the last frame that we enqueued to the processing pipeline and compare the timestamps of new frames with the timestamp of this frame, dropping the new frames if they have a lower timestamp. % TODO: Note that as opposed to subsection 2.2.2, we don't use a jitter buffer, because it doesn't make sense % TODO: Explain why

% TODO: find a better name
\section{Snapping video forward}\label{section:snapping_video_forward}
Our goal is to render new media immediately, such that plackback jumps from the old media forward to the new media. Recall, however, that the player, as we first described in \autoref{section:baseline_client}, times the rendering of frames using the relative frame timestamp. Using Algorithm \ref{alg:time_until_frame} to time the rendering of new media, would lead to the player waiting for the frame's timestamp before rendering the new GoPs, and thereby not decreasing the latency. On the one hand, we want to render new media immediately. On the other hand, we want to buffer and time consecutive frames according to their presentation time for smoother playback. To achieve this, we compare the presentation timestamp of the previously rendered frame and the frame that we are about to render. If there is a discontinuity between the presentation timestamps, we render the frame immediately, otherwise we time the rendering of the frame.

To implement this, the client keeps track of the timestamp of the last frame that was rendered and we adapt Algorithm \ref{alg:time_until_frame}, so that it first checks if there is a discontinuity between $frameTimestamp$ and the timestamp of the last rendered frame. If there is a discontinuity, $calculateTimeUntilFrame$ returns 0, so that the frame is rendered immediately. Note that if there is a discontinuity, we also need to update $resumedPlayingAt$ so that we use the correct media time as the reference when timing the next frames.

% TODO: Implement it, and measure it properly. Expand on this idea. If i do end up doing this, turn this into its own chapter.
\section{Combining both strategies}
Ideally, we want the live streaming system to handle both small drops in the network bandwidth and short spikes effectively. Although, we haven't had the time to test our hypothesis, we believe that combining both approaches lead to the most optimal performance. In this section, we describe how we would combine deprioritize B-frames and skip old GoPs. We begin by presenting a new priority scheme that integrates the new over old policy with the priority scheme of deprioritize B-frames. We then propose a way to time frames in the client.

Just like in \autoref{section:deprioritizing_b_frames} we create a track for the I-, and P-frames and a track for the B-frames. Now instead of using a stream per track for the base layer, we use a stream per GoP, and prioritize new GoPs over old GoPs. Again, we must assign the highest priorities to the streams in the base layer. For this purpose, we divide the value range of the priority values into two ranges. The values in the highest range are for the streams in the base layer, while the lowest range is for the B-frames. To make optimal use of the available bits, one can allocate more bits for the latter, since the number of B-frames is in general higher than the number of GoPs.

We would like to note that we are aware that this approach might not be viable with certain QUIC implementations, because the prioritization implementation does not use enough bits. quinn-rs for example uses 32 bits for the priority value. These are not enough bits for long duration streams. % TODO: Give a back-of-the-envelope calculation type of example

Regarding the client, we use the approach described in \ref{section:snapping_video_forward} with a small modification. Rather than rendering frames instantly if there is any discontinuity, we time frames if the discontinuity is bigger than a predefined threshold. This is because frames from the base layer might have discontinuities, if we drop B-frames, and we don't want to render them immediately. % TODO: Expand on this. The question is what's the threshold? When are two frames too far apart, such that we should render the later frame instantly. % TODO: Finish this part
